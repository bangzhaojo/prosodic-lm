{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cd0fbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, Manager, cpu_count\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca9bd687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"/shared/3/projects/benlitterer/podcastData/processed/mayJune/mayJuneMetadata.jsonl\"\n",
    "\n",
    "output_path = \"/shared/3/projects/bangzhao/prosodic_embeddings/podcast_prosodic_features/prosodic_features.csv\"\n",
    "metadata_path = \"/shared/3/projects/benlitterer/podcastData/processed/mayJune/mayJuneMetadata.jsonl\"\n",
    "metadata_path2 = \"/shared/3/projects/benlitterer/podcastData/processed/floydMonth/floydMonthDataClean.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "654fddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json_objects(file_path, kmeans, output_csv_path):\n",
    "    all_features = []  # List to hold all DataFrames\n",
    "    batch_size = 100  # Number of items before saving to CSV\n",
    "    count = 0  # Counter for processed items\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        total_lines = sum(1 for line in file)\n",
    "        file.seek(0)  # Reset file pointer to the beginning\n",
    "\n",
    "        for i, line in enumerate(tqdm(file, total=total_lines, desc=\"Processing JSON Lines\")):\n",
    "            try:\n",
    "                json_object = json.loads(line)\n",
    "                prosodic_path = \"/shared/3/projects/benlitterer/podcastData/prosodyMerged/floydMonth\" + json_object['potentialOutPath']\n",
    "                prosodic_path2 = \"/shared/3/projects/benlitterer/podcastData/prosodyMerged/mayJuneRemaining\" + json_object['potentialOutPath']\n",
    "                \n",
    "                try:\n",
    "                    prosodic_feature = pd.read_csv(prosodic_path)\n",
    "                except:\n",
    "                    prosodic_feature = pd.read_csv(prosodic_path2)\n",
    "                \n",
    "                prosodic_feature = prosodic_feature.dropna(subset=['mfcc1_sma3', 'mfcc2_sma3', 'mfcc3_sma3', 'mfcc4_sma3', 'F0semitoneFrom27.5Hz_sma3nz', 'F1frequency_sma3nz'])\n",
    "                X = prosodic_feature[['mfcc1_sma3', 'mfcc2_sma3', 'mfcc3_sma3', 'mfcc4_sma3', 'F0semitoneFrom27.5Hz_sma3nz', 'F1frequency_sma3nz']]\n",
    "                prosodic_feature = prosodic_feature[['content']]\n",
    "                prosodic_feature['id'] = i\n",
    "                \n",
    "                all_features.append((prosodic_feature, X))\n",
    "                count += 1\n",
    "                if count % batch_size == 0:\n",
    "                    combined_features = pd.concat([item[0] for item in all_features], ignore_index=True)\n",
    "                    combined_X = pd.concat([item[1] for item in all_features], ignore_index=True)\n",
    "                    \n",
    "                    labels = kmeans.predict(combined_X)\n",
    "                    combined_features['cluster_id'] = labels\n",
    "                    \n",
    "                    combined_features.to_csv(output_csv_path, mode='a', header=not bool(count // batch_size), index=False)\n",
    "                    all_features.clear()  # Clear the list for the next batch\n",
    "            \n",
    "            except Exception as e:\n",
    "                # print(f\"Error processing line {i}: {e}\")\n",
    "                continue\n",
    "                \n",
    "    # Process any remaining items in the list after the loop ends\n",
    "    if all_features:\n",
    "        combined_features = pd.concat([item[0] for item in all_features], ignore_index=True)\n",
    "        combined_X = pd.concat([item[1] for item in all_features], ignore_index=True)\n",
    "        labels = kmeans.predict(combined_X)\n",
    "        combined_features['cluster_id'] = labels\n",
    "        combined_features.to_csv(output_csv_path, mode='a', header=not bool(count // batch_size), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd7096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans = joblib.load('/shared/3/projects/bangzhao/prosodic_embeddings/sample/kmeans/kmeans_plusplus_5k_1000.pkl')\n",
    "\n",
    "# get_json_objects(metadata_path, kmeans, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d62ab2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_line(line_info):\n",
    "    line, i = line_info\n",
    "    try:\n",
    "        json_object = json.loads(line)\n",
    "        prosodic_path = \"/shared/3/projects/benlitterer/podcastData/prosodyMerged/floydMonth\" + json_object['potentialOutPath']\n",
    "        prosodic_path2 = \"/shared/3/projects/benlitterer/podcastData/prosodyMerged/mayJuneRemaining\" + json_object['potentialOutPath']\n",
    "        \n",
    "        try:\n",
    "            prosodic_feature = pd.read_csv(prosodic_path)\n",
    "        except:\n",
    "            prosodic_feature = pd.read_csv(prosodic_path2)\n",
    "        \n",
    "        prosodic_feature = prosodic_feature.dropna(subset=['mfcc1_sma3', 'mfcc2_sma3', 'mfcc3_sma3', 'mfcc4_sma3', 'F0semitoneFrom27.5Hz_sma3nz', 'F1frequency_sma3nz'])\n",
    "        X = prosodic_feature[['mfcc1_sma3', 'mfcc2_sma3', 'mfcc3_sma3', 'mfcc4_sma3', 'F0semitoneFrom27.5Hz_sma3nz', 'F1frequency_sma3nz']]\n",
    "        prosodic_feature = prosodic_feature[['content']]\n",
    "        prosodic_feature['id'] = i\n",
    "        \n",
    "        return prosodic_feature, X\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def save_to_csv(features, kmeans, output_csv_path, header):\n",
    "    combined_features = pd.concat([item[0] for item in features], ignore_index=True)\n",
    "    combined_X = pd.concat([item[1] for item in features], ignore_index=True)\n",
    "    \n",
    "    labels = kmeans.predict(combined_X)\n",
    "    combined_features['cluster_id'] = labels\n",
    "    \n",
    "    combined_features.to_csv(output_csv_path, mode='a', header=header, index=False)\n",
    "\n",
    "def get_json_objects(file_path, kmeans, output_csv_path, num_cores):\n",
    "    batch_size = 100\n",
    "    count = 0\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        total_lines = sum(1 for line in file)\n",
    "        file.seek(0)\n",
    "        \n",
    "        manager = Manager()\n",
    "        with Pool(processes=num_cores) as pool:\n",
    "            all_features = []\n",
    "            for i, result in enumerate(tqdm(pool.imap(process_line, zip(file, range(total_lines))), total=total_lines, desc=\"Processing JSON Lines\")):\n",
    "                if result:\n",
    "                    all_features.append(result)\n",
    "                    count += 1\n",
    "                    if count % batch_size == 0:\n",
    "                        save_to_csv(all_features, kmeans, output_csv_path, not bool(count // batch_size))\n",
    "                        all_features.clear()\n",
    "            \n",
    "            if all_features:\n",
    "                save_to_csv(all_features, kmeans, output_csv_path, not bool(count // batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24666134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans = joblib.load('/shared/3/projects/bangzhao/prosodic_embeddings/sample/kmeans/kmeans_plusplus_5k_1000.pkl')\n",
    "\n",
    "# get_json_objects(metadata_path, kmeans, output_path, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bd1ddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch):\n",
    "    results = []\n",
    "    for i, line in batch:\n",
    "        try:\n",
    "            json_object = json.loads(line)\n",
    "            prosodic_path = \"/shared/3/projects/benlitterer/podcastData/prosodyMerged/floydMonth\" + json_object['potentialOutPath']\n",
    "            prosodic_path2 = \"/shared/3/projects/benlitterer/podcastData/prosodyMerged/mayJuneRemaining\" + json_object['potentialOutPath']\n",
    "            \n",
    "            try:\n",
    "                prosodic_feature = pd.read_csv(prosodic_path)\n",
    "            except:\n",
    "                prosodic_feature = pd.read_csv(prosodic_path2)\n",
    "            \n",
    "            prosodic_feature = prosodic_feature.dropna(subset=['mfcc1_sma3', 'mfcc2_sma3', 'mfcc3_sma3', 'mfcc4_sma3', 'F0semitoneFrom27.5Hz_sma3nz', 'F1frequency_sma3nz'])\n",
    "            X = prosodic_feature[['mfcc1_sma3', 'mfcc2_sma3', 'mfcc3_sma3', 'mfcc4_sma3', 'F0semitoneFrom27.5Hz_sma3nz', 'F1frequency_sma3nz']]\n",
    "            prosodic_feature = prosodic_feature[['content']]\n",
    "            prosodic_feature['id'] = i\n",
    "            \n",
    "            results.append((prosodic_feature, X))\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    return results\n",
    "\n",
    "def save_to_csv(features, kmeans, output_csv_path, header):\n",
    "    combined_features = pd.concat([item[0] for item in features], ignore_index=True)\n",
    "    combined_X = pd.concat([item[1] for item in features], ignore_index=True)\n",
    "    \n",
    "    labels = kmeans.predict(combined_X)\n",
    "    combined_features['cluster_id'] = labels\n",
    "    \n",
    "    combined_features.to_csv(output_csv_path, mode='a', header=header, index=False)\n",
    "\n",
    "def get_json_objects(file_path, kmeans, output_csv_path):\n",
    "    batch_size = 100\n",
    "    count = 0\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        total_lines = sum(1 for line in file)\n",
    "        file.seek(0)\n",
    "        \n",
    "        lines = list(enumerate(file))\n",
    "        batches = [lines[i:i + batch_size] for i in range(0, total_lines, batch_size)]\n",
    "        \n",
    "        manager = Manager()\n",
    "        with Pool(processes=6) as pool:\n",
    "            for batch in tqdm(pool.imap(process_batch, batches), total=len(batches), desc=\"Processing JSON Lines\"):\n",
    "                if batch:\n",
    "                    save_to_csv(batch, kmeans, output_csv_path, not bool(count // batch_size))\n",
    "                    count += len(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7389f5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans = joblib.load('/shared/3/projects/bangzhao/prosodic_embeddings/sample/kmeans/kmeans_plusplus_5k_1000.pkl')\n",
    "\n",
    "# get_json_objects(metadata_path, kmeans, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e470fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_FILE = '/shared/3/projects/bangzhao/prosodic_embeddings/podcast_prosodic_features/checkpoint.txt'\n",
    "\n",
    "def process_batch(batch):\n",
    "    results = []\n",
    "    for i, line in batch:\n",
    "        try:\n",
    "            json_object = json.loads(line)\n",
    "            prosodic_path = \"/shared/3/projects/benlitterer/podcastData/prosodyMerged/floydMonth\" + json_object['potentialOutPath']\n",
    "            prosodic_path2 = \"/shared/3/projects/benlitterer/podcastData/prosodyMerged/mayJuneRemaining\" + json_object['potentialOutPath']\n",
    "            \n",
    "            try:\n",
    "                prosodic_feature = pd.read_csv(prosodic_path)\n",
    "            except:\n",
    "                prosodic_feature = pd.read_csv(prosodic_path2)\n",
    "            \n",
    "            prosodic_feature = prosodic_feature.dropna(subset=['mfcc1_sma3', 'mfcc2_sma3', 'mfcc3_sma3', 'mfcc4_sma3', 'F0semitoneFrom27.5Hz_sma3nz', 'F1frequency_sma3nz'])\n",
    "            X = prosodic_feature[['mfcc1_sma3', 'mfcc2_sma3', 'mfcc3_sma3', 'mfcc4_sma3', 'F0semitoneFrom27.5Hz_sma3nz', 'F1frequency_sma3nz']]\n",
    "            prosodic_feature = prosodic_feature[['content']]\n",
    "            prosodic_feature['id'] = i\n",
    "            # prosodic_feature['path'] = json_object[potentialOutPath]\n",
    "            \n",
    "            results.append((prosodic_feature, X))\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    return results\n",
    "\n",
    "def save_to_csv(features, kmeans, output_csv_path, header):\n",
    "    combined_features = pd.concat([item[0] for item in features], ignore_index=True)\n",
    "    combined_X = pd.concat([item[1] for item in features], ignore_index=True)\n",
    "    \n",
    "    labels = kmeans.predict(combined_X)\n",
    "    combined_features['cluster_id'] = labels\n",
    "    \n",
    "    combined_features.to_csv(output_csv_path, mode='a', header=header, index=False)\n",
    "\n",
    "def load_checkpoint():\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        with open(CHECKPOINT_FILE, 'r') as file:\n",
    "            return int(file.read().strip())\n",
    "    return 0\n",
    "\n",
    "def save_checkpoint(batch_index):\n",
    "    with open(CHECKPOINT_FILE, 'w') as file:\n",
    "        file.write(str(batch_index))\n",
    "\n",
    "def get_json_objects(file_path, kmeans, output_csv_path):\n",
    "    batch_size = 200\n",
    "    count = 0\n",
    "    start_batch = load_checkpoint()\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        total_lines = sum(1 for line in file)\n",
    "        file.seek(0)\n",
    "        \n",
    "        lines = list(enumerate(file))\n",
    "        batches = [lines[i:i + batch_size] for i in range(0, total_lines, batch_size)]\n",
    "        \n",
    "        manager = Manager()\n",
    "        with Pool(processes=6) as pool:\n",
    "            for batch_index, batch in enumerate(tqdm(pool.imap(process_batch, batches), total=len(batches), desc=\"Processing JSON Lines\")):\n",
    "                if batch_index < start_batch:\n",
    "                    continue  # Skip batches that have already been processed\n",
    "                if batch:\n",
    "                    save_to_csv(batch, kmeans, output_csv_path, not bool(count // batch_size))\n",
    "                    count += len(batch)\n",
    "                    save_checkpoint(batch_index + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50d1e5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON Lines: 100%|████████████████████████████████████████████████████| 16436/16436 [8:02:24<00:00,  1.76s/it]\n"
     ]
    }
   ],
   "source": [
    "kmeans = joblib.load('/shared/3/projects/bangzhao/prosodic_embeddings/sample/kmeans/kmeans_plusplus_5k_1000.pkl')\n",
    "\n",
    "get_json_objects(metadata_path, kmeans, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf7204c",
   "metadata": {},
   "source": [
    "## id path mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0873115d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_outpath_map(file_path):\n",
    "    mapping = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        total_lines = sum(1 for line in file)\n",
    "        file.seek(0)  # Reset file pointer to the beginning\n",
    "        \n",
    "        for i, line in enumerate(tqdm(file, total=total_lines, desc=\"Processing JSON Lines\")):\n",
    "            try:\n",
    "                json_object = json.loads(line)\n",
    "                mapping[i] = json_object['potentialOutPath']\n",
    "            except Exception as e:\n",
    "                # Optionally, handle the error (e.g., log it)\n",
    "                continue\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acb0c6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON Lines: 100%|███████████████████████████████████████████████| 1643516/1643516 [01:01<00:00, 26657.73it/s]\n"
     ]
    }
   ],
   "source": [
    "mapping = id_outpath_map(metadata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f464e4c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mapping\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/shared/3/projects/bangzhao/prosodic_embeddings/podcast_prosodic_features/id_outpath_map.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [
    "mapping.to_csv('/shared/3/projects/bangzhao/prosodic_embeddings/podcast_prosodic_features/id_outpath_map.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd1f0e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
