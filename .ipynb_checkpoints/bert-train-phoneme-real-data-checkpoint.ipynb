{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f03def84-41e2-4c11-b27c-b5d9e4646a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "from transformers import AdamW\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import *\n",
    "from tokenizers import *\n",
    "\n",
    "from bertPhoneme import BertEmbeddingsV2, BertModelV2, BertForMaskedLMV2, BertConfigV2, MaskedLMWithProsodyOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9c72171e-21e2-4d69-a215-4ff3871dbe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc1e7ec1-5626-493a-9050-b422ea2dbdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/shared/3/projects/bangzhao/prosodic_embeddings/merge/training_data/output_part_1.jsonl\"\n",
    "phonemes = set()\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 10000:\n",
    "            break\n",
    "        phonemes.update(set(json.loads(line)['phoneme']))\n",
    "    print(len(phonemes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "691b952e-554d-4235-b331-de4da37ca8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Z', 'AE1', 'W', 'G', 'AA0', 'AA2', 'R', 'AO1', 'D', 'IH2', 'AO2', 'Y', 'NG', 'EY2', 'DH', 'K', 'V', 'ER2', 'UW1', 'EY1', 'AA1', 'EH0', 'OY2', 'AY2', 'OY1', 'AE2', 'UH1', 'AH1', 'HH', 'JH', 'SH', 'SIL', 'UW2', 'ZH', 'AY1', 'AW0', 'N', 'UW0', 'IY1', 'EH2', 'OW1', 'M', 'ER0', 'OW0', 'TH', 'EY0', 'UH0', 'AW1', 'AH0', 'L', 'T', 'AW2', 'EH1', 'AY0', 'B', 'IY0', 'F', 'P', 'ER1', 'IH1', 'IH0', 'AO0', 'OW2', 'spn', 'OY0', 'UNK', 'UH2', 'S', 'IY2', 'CH', 'AH2', 'AE0']\n"
     ]
    }
   ],
   "source": [
    "phonemes.update(['UNK', 'SIL'])\n",
    "print(list(phonemes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9e602f1-9590-4cb4-9f42-805508a859c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneme_vocab = {p: i for i, p in enumerate(phonemes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4abb52c-3b70-4419-9011-19c696bb31d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Z': 0, 'AE1': 1, 'W': 2, 'G': 3, 'AA0': 4, 'AA2': 5, 'R': 6, 'AO1': 7, 'D': 8, 'IH2': 9, 'AO2': 10, 'Y': 11, 'NG': 12, 'EY2': 13, 'DH': 14, 'K': 15, 'V': 16, 'ER2': 17, 'UW1': 18, 'EY1': 19, 'AA1': 20, 'EH0': 21, 'OY2': 22, 'AY2': 23, 'OY1': 24, 'AE2': 25, 'UH1': 26, 'AH1': 27, 'HH': 28, 'JH': 29, 'SH': 30, 'SIL': 31, 'UW2': 32, 'ZH': 33, 'AY1': 34, 'AW0': 35, 'N': 36, 'UW0': 37, 'IY1': 38, 'EH2': 39, 'OW1': 40, 'M': 41, 'ER0': 42, 'OW0': 43, 'TH': 44, 'EY0': 45, 'UH0': 46, 'AW1': 47, 'AH0': 48, 'L': 49, 'T': 50, 'AW2': 51, 'EH1': 52, 'AY0': 53, 'B': 54, 'IY0': 55, 'F': 56, 'P': 57, 'ER1': 58, 'IH1': 59, 'IH0': 60, 'AO0': 61, 'OW2': 62, 'spn': 63, 'OY0': 64, 'UNK': 65, 'UH2': 66, 'S': 67, 'IY2': 68, 'CH': 69, 'AH2': 70, 'AE0': 71}\n"
     ]
    }
   ],
   "source": [
    "print(phoneme_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4c403dc3-7c84-4c7c-b50c-fd1db50a124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneme_vocab_size = len(phoneme_vocab) \n",
    "mask_token_id = phoneme_vocab[\"SIL\"]\n",
    "pad_token_id = 72\n",
    "pad_cluster_id = 200\n",
    "\n",
    "class HuggingFacePhonemeDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, vocab, mask_prob=0.2, max_length=128):\n",
    "        self.dataset = hf_dataset\n",
    "        self.vocab = vocab\n",
    "        self.mask_prob = mask_prob\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "\n",
    "        phonemes = sample[\"phoneme\"]\n",
    "        prosody_ids = sample[\"prosody_id_200\"]\n",
    "\n",
    "        input_ids = [self.vocab.get(p, self.vocab[\"UNK\"]) for p in phonemes][:self.max_length]\n",
    "        prosody_ids = prosody_ids[:self.max_length]\n",
    "\n",
    "        labels = input_ids.copy()\n",
    "        for i in range(len(input_ids)):\n",
    "            if random.random() < self.mask_prob:\n",
    "                labels[i] = input_ids[i]\n",
    "                input_ids[i] = self.vocab[\"SIL\"]\n",
    "\n",
    "        pad_length = self.max_length - len(input_ids)\n",
    "        input_ids.extend([pad_token_id] * pad_length)\n",
    "        labels.extend([-100] * pad_length)\n",
    "        prosody_ids.extend([pad_cluster_id] * pad_length)\n",
    "        attention_mask = [1] * (self.max_length - pad_length) + [0] * pad_length\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "            \"prosody_ids\": torch.tensor(prosody_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b1f0459-c036-4a1c-8a6e-47de4d38aa5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19c3d7eb574494d91249ed47234a47d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "955f296aeeb04b339c904d2f3ca68ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf_dataset = load_dataset(\"json\", data_files=\"/shared/3/projects/bangzhao/prosodic_embeddings/merge/training_data/output_part_1.jsonl\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0ca94bbc-1c05-42c6-baf8-25be3fec747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HuggingFacePhonemeDataset(hf_dataset, phoneme_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a01d475a-223d-4671-b5f7-924201779243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([34, 41, 50, 59, 56,  1, 49, 48, 36,  2, 60, 14, 14, 48, 63, 63,  0, 31,\n",
       "         60, 36, 49, 34, 50, 48, 31, 14, 27,  6, 31, 67, 48, 36, 31, 67, 48, 57,\n",
       "          6, 38, 41, 15,  7,  6, 50,  8, 60, 67, 59, 33, 48, 36, 57,  6, 43, 28,\n",
       "         59, 54, 48, 31, 60, 12, 60, 41, 57, 49, 31, 41, 48, 36, 31,  8, 60, 67,\n",
       "         31,  6,  9, 41, 48, 31, 19, 31, 31, 36, 48,  3, 52, 36, 67, 50, 31, 62,\n",
       "         41, 43, 67, 52, 15, 31, 31, 48, 49,  0, 48, 31,  8, 63, 67, 52, 31, 48,\n",
       "         31, 42, 31, 20, 30, 31,  7, 31, 55, 48, 31, 31, 48, 31, 26,  6, 55, 31,\n",
       "         36, 50]),\n",
       " 'labels': tensor([34, 41, 50, 59, 56,  1, 49, 48, 36,  2, 60, 14, 14, 48, 63, 63,  0, 63,\n",
       "         60, 36, 49, 34, 50, 48, 16, 14, 27,  6, 38, 67, 48, 36, 50, 67, 48, 57,\n",
       "          6, 38, 41, 15,  7,  6, 50,  8, 60, 67, 59, 33, 48, 36, 57,  6, 43, 28,\n",
       "         59, 54, 48, 50, 60, 12, 60, 41, 57, 49, 24, 41, 48, 36, 50,  8, 60, 67,\n",
       "         15,  6,  9, 41, 48, 36, 19, 30, 48, 36, 48,  3, 52, 36, 67, 50, 28, 62,\n",
       "         41, 43, 67, 52, 15, 30, 32, 48, 49,  0, 48, 36,  8, 63, 67, 52, 36, 48,\n",
       "         50, 42, 29, 20, 30, 28,  7, 49, 55, 48, 16, 41, 48,  0, 26,  6, 55,  7,\n",
       "         36, 50]),\n",
       " 'prosody_ids': tensor([118,  69,  69,  71,  56,  47, 105, 153, 109,  99, 136,   9,   7,  32,\n",
       "           7,  56, 113,  13, 166,  41, 117,  30, 169,  11,  51,  33,  76,  38,\n",
       "          96,   3,  76, 151,   7, 151,  15,  58,  98,  96,  88,   3, 114,  51,\n",
       "         183,   8, 153, 166, 131, 184,  76,  23,  56, 106, 118, 102,  11, 105,\n",
       "         125, 154, 128, 183,   3, 173, 161,   7,   8,  81,   1, 152,   7, 125,\n",
       "         179, 152,  99,  69,  69,  75, 122,  78,  51,  23, 153,   3, 102, 147,\n",
       "         141,  99,  75, 130, 129, 105, 105,  62,  52, 185,  58, 152, 105,  69,\n",
       "         156,  11, 124, 153, 158,   7, 166,  27, 128, 153, 173,  30,  58, 131,\n",
       "         152, 184, 175,  48, 144,  57, 183,  74,  49, 152, 167,  39, 123,  71,\n",
       "         191, 175]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4261f08a-8c9d-4d70-b67c-7608045fe1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e1c18a2e-ce10-4a97-8cbf-e757a28ae164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[60, 50, 67,  ...,  6, 68, 48],\n",
      "        [ 3, 52, 31,  ..., 31, 36,  8],\n",
      "        [28, 21, 31,  ..., 31, 56,  7],\n",
      "        ...,\n",
      "        [28, 48, 49,  ..., 50, 31, 31],\n",
      "        [28, 31, 14,  ..., 31, 15, 30],\n",
      "        [63,  6, 68,  ..., 36, 14, 48]]), 'labels': tensor([[60, 50, 67,  ...,  6, 68, 48],\n",
      "        [ 3, 52, 50,  ...,  1, 36,  8],\n",
      "        [28, 21, 49,  ..., 49, 56,  7],\n",
      "        ...,\n",
      "        [28, 48, 49,  ..., 50, 41, 42],\n",
      "        [28, 34, 14,  ...,  1, 15, 30],\n",
      "        [63,  6, 68,  ..., 36, 14, 48]]), 'prosody_ids': tensor([[  7,  60, 153,  ..., 138, 154,  68],\n",
      "        [153, 153,  77,  ...,   7,  46, 186],\n",
      "        [ 76,  41, 153,  ..., 174,   7,  49],\n",
      "        ...,\n",
      "        [179, 153, 153,  ...,  29, 131, 116],\n",
      "        [195, 111,  99,  ...,   7, 174,  23],\n",
      "        [  7, 179,   0,  ...,  46, 133,  94]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9fbbbaf0-b9da-4649-be17-47904851752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(phoneme_vocab.keys())\n",
    "max_length = 128\n",
    "\n",
    "\n",
    "model_config = BertConfigV2(\n",
    "    vocab_size=vocab_size,\n",
    "    hidden_size=128,\n",
    "    num_hidden_layers=2,\n",
    "    num_attention_heads=2,\n",
    "    intermediate_size=512,\n",
    "    max_position_embeddings=max_length,\n",
    "    prosody_cluster_size=3\n",
    ")\n",
    "\n",
    "model = BertForMaskedLMV2(config=model_config)\n",
    "\n",
    "# BERT-Base\t768\t12\t12\t3072\n",
    "# BERT-Small 512\t4\t8\t2048\n",
    "# BERT-Mini\t256\t4\t4\t1024\n",
    "# BERT-Tiny\t128\t2\t2\t512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "74f0028c-cbbb-418d-a55a-c783e30472b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLMV2(\n",
       "  (bert): BertModelV2(\n",
       "    (embeddings): BertEmbeddingsV2(\n",
       "      (word_embeddings): Embedding(72, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(128, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (prosody_embeddings): Embedding(3, 128)\n",
       "      (conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=128, out_features=72, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (prosody_head): Linear(in_features=128, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "53cef04b-4a26-46aa-a4be-344ff7a9a255",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Define loss functions\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/site-packages/transformers/modeling_utils.py:3162\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   3158\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3159\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3160\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3161\u001b[0m         )\n\u001b[0;32m-> 3162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/site-packages/torch/nn/modules/module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/site-packages/torch/nn/modules/module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1321\u001b[0m             device,\n\u001b[1;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m             non_blocking,\n\u001b[1;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1327\u001b[0m         device,\n\u001b[1;32m   1328\u001b[0m         dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1329\u001b[0m         non_blocking,\n\u001b[1;32m   1330\u001b[0m     )\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Define loss functions\n",
    "mlm_loss_fn = nn.CrossEntropyLoss(ignore_index=-100)  # MLM Loss\n",
    "prosody_loss_fn = nn.CrossEntropyLoss()  # Prosody Classification Loss\n",
    "\n",
    "num_epochs = 5\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        prosody_ids = batch[\"prosody_ids\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, prosody_ids=prosody_ids)\n",
    "\n",
    "        # Compute losses\n",
    "        mlm_loss = mlm_loss_fn(outputs.logits.view(-1, phoneme_vocab_size), labels.view(-1))\n",
    "        prosody_loss = prosody_loss_fn(outputs.prosody_logits.view(-1, model.config.prosody_cluster_size), prosody_ids.view(-1))\n",
    "\n",
    "        # Combine losses\n",
    "        total_batch_loss = mlm_loss + prosody_loss\n",
    "        total_batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += total_batch_loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Loss = {total_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "481f2209-7014-4f9f-b9f4-5ff13ba6cedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = [\n",
    "    ([\"DH\", \"S\", \"IH\", \"Z\", \"AH\", \"T\", \"EH\", \"S\", \"T\"], [0, 1, 2, 2, 1, 1, 0, 1, 1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5bf7c283-8bef-4a00-8ead-5ac2467f74c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 9, 28, 16, 40,  2, 30, 10, 40, 30,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0]),\n",
       " 'labels': tensor([   9,   28,   16,   37,    2,   30,   10,   28,   30, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100]),\n",
       " 'prosody_ids': tensor([0, 1, 2, 2, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = PhonemeProsodyDataset(test_dataset, phoneme_vocab)\n",
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9dcd16b5-46f4-4a73-a6e5-460c0bc641d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "input_ids = test_dataset[0][\"input_ids\"].unsqueeze(0).to(device)     # shape: [1, seq_len]\n",
    "labels = test_dataset[0][\"labels\"].unsqueeze(0).to(device)           # shape: [1, seq_len]\n",
    "prosody_ids = test_dataset[0][\"prosody_ids\"].unsqueeze(0).to(device) # shape: [1, seq_len]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, prosody_ids=prosody_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9c65b71b-12b7-4dc6-a2a0-59353a8e12b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 predictions for masked tokens (SIL):\n",
      "\n",
      "Token 2 (Masked Position):\n",
      "  Top 5 Phoneme Predictions:\n",
      "    AH: 0.4596\n",
      "    AE: 0.0711\n",
      "    OW: 0.0620\n",
      "    AO: 0.0358\n",
      "    NG: 0.0338\n",
      "  Top Prosody Predictions:\n",
      "    Cluster 1: 0.9979\n",
      "    Cluster 0: 0.0017\n",
      "    Cluster 2: 0.0005\n",
      "----------------------------------------\n",
      "Token 7 (Masked Position):\n",
      "  Top 5 Phoneme Predictions:\n",
      "    EH: 0.2366\n",
      "    W: 0.2217\n",
      "    IH: 0.0439\n",
      "    B: 0.0438\n",
      "    EY: 0.0413\n",
      "  Top Prosody Predictions:\n",
      "    Cluster 0: 0.9802\n",
      "    Cluster 2: 0.0134\n",
      "    Cluster 1: 0.0065\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Assume you have access to phoneme_vocab and its inverse mapping\n",
    "id2phoneme = {v: k for k, v in phoneme_vocab.items()}\n",
    "phoneme_logits = outputs.logits[0]  # shape: [seq_len, vocab_size]\n",
    "prosody_logits = outputs.prosody_logits[0]  # shape: [seq_len, prosody_cluster_size]\n",
    "\n",
    "print(\"Top 5 predictions for masked tokens (SIL):\\n\")\n",
    "\n",
    "for i in range(input_ids.shape[1]):  # loop over tokens in sequence\n",
    "    if input_ids[0, i].item() != phoneme_vocab[\"SIL\"]:\n",
    "        continue  # Only predict for masked tokens\n",
    "\n",
    "    print(f\"Token {i + 1} (Masked Position):\")\n",
    "\n",
    "    # ==== Phoneme Prediction ====\n",
    "    phoneme_probs = torch.softmax(phoneme_logits[i], dim=-1)\n",
    "    top5_phoneme = torch.topk(phoneme_probs, 5)\n",
    "    print(\"  Top 5 Phoneme Predictions:\")\n",
    "    for j in range(5):\n",
    "        pid = top5_phoneme.indices[j].item()\n",
    "        prob = top5_phoneme.values[j].item()\n",
    "        print(f\"    {id2phoneme[pid]}: {prob:.4f}\")\n",
    "\n",
    "    # ==== Prosody Prediction ====\n",
    "    prosody_probs = torch.softmax(prosody_logits[i], dim=-1)\n",
    "    top5_prosody = torch.topk(prosody_probs, min(5, prosody_probs.size(-1)))\n",
    "    print(\"  Top Prosody Predictions:\")\n",
    "    for j in range(top5_prosody.indices.size(0)):\n",
    "        pid = top5_prosody.indices[j].item()\n",
    "        prob = top5_prosody.values[j].item()\n",
    "        print(f\"    Cluster {pid}: {prob:.4f}\")\n",
    "    \n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7f1f90-8e65-40ac-87bd-c86d9f1ee4f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
