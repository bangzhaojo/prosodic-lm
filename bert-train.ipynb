{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3be6bbae-de3a-401d-9061-6acc3290b2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 00:46:02.333351: I tensorflow/core/platform/cpu_feature_guard.cc:181] Beginning TensorFlow 2.15, this package will be updated to install stock TensorFlow 2.15 alongside Intel's TensorFlow CPU extension plugin, which provides all the optimizations available in the package and more. If a compatible version of stock TensorFlow is present, only the extension will get installed. No changes to code or installation setup is needed as a result of this change.\n",
      "More information on Intel's optimizations for TensorFlow, delivered as TensorFlow extension plugin can be viewed at https://github.com/intel/intel-extension-for-tensorflow.\n",
      "2025-03-03 00:46:02.333527: I tensorflow/core/platform/cpu_feature_guard.cc:192] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# https://thepythoncode.com/article/pretraining-bert-huggingface-transformers-in-python\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "from datasets import *\n",
    "from transformers import *\n",
    "from tokenizers import *\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23e770cd-1e5e-43ff-9e68-ad2f89a1269e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cc_news\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffd7b261-4935-4cb6-b707-fd8513340db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['title', 'text', 'domain', 'date', 'description', 'url', 'image_url'],\n",
       "     num_rows: 637416\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['title', 'text', 'domain', 'date', 'description', 'url', 'image_url'],\n",
       "     num_rows: 70825\n",
       " }))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = dataset.train_test_split(test_size=0.1)\n",
    "d[\"train\"], d[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a0a56bc-2fd2-48a3-b342-a21e468a52b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for t in d[\"train\"][\"text\"][:1]:\n",
    "#   print(t)\n",
    "#   print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2689fa-75c1-44f8-87ff-bebab0f434c5",
   "metadata": {},
   "source": [
    "## Train a Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a16eebc-b696-4728-8b14-e692e0130b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to train the tokenizer from scratch (especially if you have custom\n",
    "# dataset loaded as datasets object), then run this cell to save it as files\n",
    "# but if you already have your custom data as text files, there is no point using this\n",
    "def dataset_to_text(dataset, output_filename=\"data.txt\"):\n",
    "  \"\"\"Utility function to save dataset text to disk,\n",
    "  useful for using the texts to train the tokenizer \n",
    "  (as the tokenizer accepts files)\"\"\"\n",
    "  with open(output_filename, \"w\") as f:\n",
    "    for t in dataset[\"text\"]:\n",
    "      print(t, file=f)\n",
    "\n",
    "# save the training set to train.txt\n",
    "dataset_to_text(d[\"train\"], \"/shared/3/projects/bangzhao/prosodic_embeddings/bert_train/dataset/train.txt\")\n",
    "# save the testing set to test.txt\n",
    "dataset_to_text(d[\"test\"], \"/shared/3/projects/bangzhao/prosodic_embeddings/bert_train/dataset/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c50a484-ee59-4174-9192-39cc604526e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "781a9f9a5f6e4ee1997734a465803f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Lines: 0 lines [00:00, ? lines/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in corpus: 3291715\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count unique words in corpus with a progress bar\n",
    "def count_vocab(file_path):\n",
    "    word_set = set()\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, desc=\"Processing Lines\", unit=\" lines\"):\n",
    "            words = line.split()\n",
    "            word_set.update(words)  # Add unique words to set\n",
    "    return len(word_set)  # Count unique words\n",
    "\n",
    "# File path\n",
    "file_path = \"/shared/3/projects/bangzhao/prosodic_embeddings/bert_train/dataset/train.txt\"\n",
    "\n",
    "# Count and print vocabulary size\n",
    "vocab_count = count_vocab(file_path)\n",
    "print(f\"Unique words in corpus: {vocab_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dc5ebeb-e00c-47f1-acde-8c4a197c662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\n",
    "  \"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"<S>\", \"<T>\"\n",
    "]\n",
    "# if you want to train the tokenizer on both sets\n",
    "# files = [\"train.txt\", \"test.txt\"]\n",
    "# training the tokenizer on the training set\n",
    "files = [\"/shared/3/projects/bangzhao/prosodic_embeddings/bert_train/dataset/train.txt\"]\n",
    "# 30,522 vocab is BERT's default vocab size, feel free to tweak\n",
    "vocab_size = 30_522\n",
    "# maximum sequence length, lowering will result to faster training (when increasing batch size)\n",
    "max_length = 512\n",
    "# whether to truncate\n",
    "truncate_longer_samples = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e976a41-b263-4e61-a5d8-91b91f257ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize the WordPiece tokenizer\n",
    "tokenizer = BertWordPieceTokenizer()\n",
    "# train the tokenizer\n",
    "tokenizer.train(files=files, vocab_size=vocab_size, special_tokens=special_tokens)\n",
    "# enable truncation up to the maximum 512 tokens\n",
    "tokenizer.enable_truncation(max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36e1ca82-46d5-4fa3-8568-4730a378739f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/shared/3/projects/bangzhao/prosodic_embeddings/bert_train/pretrained-bert/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a03767e-5e94-4b91-8968-06ea52404ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the directory if not already there\n",
    "if not os.path.isdir(model_path):\n",
    "  os.mkdir(model_path)\n",
    "# save the tokenizer  \n",
    "tokenizer.save_model(model_path)\n",
    "# dumping some of the tokenizer config to config file, \n",
    "# including special tokens, whether to lower case and the maximum sequence length\n",
    "with open(os.path.join(model_path, \"config.json\"), \"w\") as f:\n",
    "  tokenizer_cfg = {\n",
    "      \"do_lower_case\": True,\n",
    "      \"unk_token\": \"[UNK]\",\n",
    "      \"sep_token\": \"[SEP]\",\n",
    "      \"pad_token\": \"[PAD]\",\n",
    "      \"cls_token\": \"[CLS]\",\n",
    "      \"mask_token\": \"[MASK]\",\n",
    "      \"model_max_length\": max_length,\n",
    "      \"max_len\": max_length,\n",
    "  }\n",
    "  json.dump(tokenizer_cfg, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0f3e134-7099-453a-b606-97bcd586af01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "loading configuration file /shared/3/projects/bangzhao/prosodic_embeddings/bert_train/pretrained-bert/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/shared/3/projects/bangzhao/prosodic_embeddings/bert_train/pretrained-bert/\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file /shared/3/projects/bangzhao/prosodic_embeddings/bert_train/pretrained-bert/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/shared/3/projects/bangzhao/prosodic_embeddings/bert_train/pretrained-bert/\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa68d783-b7a1-4c01-a607-05b55363f504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a787d725ca145b69383110c3615f6d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing Train Dataset:   0%|          | 0/637416 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1613daac56224ddebd26929653c27aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing Test Dataset:   0%|          | 0/70825 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def encode_with_truncation(examples):\n",
    "    \"\"\"Mapping function to tokenize the sentences with truncation\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], truncation=True, padding=\"max_length\",\n",
    "        max_length=max_length, return_special_tokens_mask=True\n",
    "    )\n",
    "\n",
    "def encode_without_truncation(examples):\n",
    "    \"\"\"Mapping function to tokenize the sentences without truncation\"\"\"\n",
    "    return tokenizer(examples[\"text\"], return_special_tokens_mask=True)\n",
    "\n",
    "# Select the appropriate function\n",
    "encode = encode_with_truncation if truncate_longer_samples else encode_without_truncation\n",
    "\n",
    "train_dataset = d[\"train\"].map(encode, batched=True, desc=\"Tokenizing Train Dataset\")\n",
    "test_dataset = d[\"test\"].map(encode, batched=True, desc=\"Tokenizing Test Dataset\")\n",
    "\n",
    "if truncate_longer_samples:\n",
    "    train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "else:\n",
    "    test_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])\n",
    "    train_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcefcc10-3c93-4e5c-92bc-df62a83d3f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'text', 'domain', 'date', 'description', 'url', 'image_url', 'input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "    num_rows: 637416\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8828077-4afb-43aa-9169-08fcc5ec5e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd06eafe14549b8843dabd6d6dc2c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 512:   0%|          | 0/637416 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36c5481fb631478596c547517cc09395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 512:   0%|          | 0/70825 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of\n",
    "# max_seq_length.\n",
    "# grabbed from: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= max_length:\n",
    "        total_length = (total_length // max_length) * max_length\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + max_length] for i in range(0, total_length, max_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "# Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a\n",
    "# remainder for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value\n",
    "# might be slower to preprocess.\n",
    "#\n",
    "# To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n",
    "# https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n",
    "if not truncate_longer_samples:\n",
    "  train_dataset = train_dataset.map(group_texts, batched=True,\n",
    "                                    desc=f\"Grouping texts in chunks of {max_length}\")\n",
    "  test_dataset = test_dataset.map(group_texts, batched=True,\n",
    "                                  desc=f\"Grouping texts in chunks of {max_length}\")\n",
    "  # convert them from lists to torch tensors\n",
    "  train_dataset.set_format(\"torch\")\n",
    "  test_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e59d66c4-e015-4b2f-9c59-4d9244cae573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73912, 8221)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "566df67d-c007-4a97-9675-ba3cb674fbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = BertConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    hidden_size=128,\n",
    "    num_hidden_layers=2,\n",
    "    num_attention_heads=2,\n",
    "    intermediate_size=512,\n",
    "    max_position_embeddings=max_length,\n",
    ")\n",
    "\n",
    "model = BertForMaskedLM(config=model_config)\n",
    "\n",
    "# BERT-Base\t768\t12\t12\t3072\n",
    "# BERT-Small 512\t4\t8\t2048\n",
    "# BERT-Mini\t256\t4\t4\t1024\n",
    "# BERT-Tiny\t128\t2\t2\t512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7f29542d-f91f-4f12-9f60-5415793de4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=128, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "218781bd-ddae-4bf2-a4ce-17a5b010711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3290236a-7588-4fe7-ad97-baa2037297fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "using `logging_steps` to initialize `eval_steps` to 1000\n",
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path,          \n",
    "    evaluation_strategy=\"steps\",    \n",
    "    overwrite_output_dir=True,      \n",
    "    num_train_epochs=3,            \n",
    "    per_device_train_batch_size=16,  # Reduce batch size for better convergence\n",
    "    gradient_accumulation_steps=2,   # Keep it small for stability\n",
    "    per_device_eval_batch_size=16,   \n",
    "    logging_steps=1000,             \n",
    "    save_steps=1000,\n",
    "    learning_rate=1e-3,              # first use 3e-3 for 6 epochs, then use 1e-3 for 6 epochs\n",
    "    warmup_steps=1000,               # Prevent instability\n",
    "    weight_decay=0.01,               # Helps generalization\n",
    "    report_to='none',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "10bbb5a2-1690-46b6-a133-c5a11da21620",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d1da2b06-b820-4d0e-bf72-cbb46febd3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask, title, description, url, image_url, text, domain, date. If special_tokens_mask, title, description, url, image_url, text, domain, date are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 73,912\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 6,930\n",
      "  Number of trainable parameters = 4,416,698\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6930' max='6930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6930/6930 15:45, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.065300</td>\n",
       "      <td>3.837000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.076800</td>\n",
       "      <td>3.838254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.076400</td>\n",
       "      <td>3.811674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.070600</td>\n",
       "      <td>3.796210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.055000</td>\n",
       "      <td>3.773549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>4.046500</td>\n",
       "      <td>3.759283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask, title, description, url, image_url, text, domain, date. If special_tokens_mask, title, description, url, image_url, text, domain, date are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8221\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /shared/3/projects/bangzhao/prosodic_embeddings/bert_train/pretrained-bert/checkpoint-1000\n",
      "Configuration saved in /shared/3/projects/bangzhao/prosodic_embeddings/bert_train/pretrained-bert/checkpoint-1000/config.json\n",
      "Model weights saved in /shared/3/projects/bangzhao/prosodic_embeddings/bert_train/pretrained-bert/checkpoint-1000/model.safetensors\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask, title, description, url, image_url, text, domain, date. If special_tokens_mask, title, description, url, image_url, text, domain, date are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8221\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /shared/3/projects/bangzhao/prosodic_embeddings/bert_train/pretrained-bert/checkpoint-2000\n",
      "Configuration saved in /shared/3/projects/bangzhao/prosodic_embeddings/bert_train/pretrained-bert/checkpoint-2000/config.json\n",
      "Model weights saved in /shared/3/projects/bangzhao/prosodic_embeddings/bert_train/pretrained-bert/checkpoint-2000/model.safetensors\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask, title, description, url, image_url, text, domain, date. If special_tokens_mask, title, description, url, image_url, text, domain, date are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8221\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /shared/3/projects/bangzhao/prosodic_embeddings/bert_train/pretrained-bert/checkpoint-3000\n",
      "Configuration saved in /shared/3/projects/bangzhao/prosodic_embeddings/bert_train/pretrained-bert/checkpoint-3000/config.json\n",
      "Model weights saved in /shared/3/projects/bangzhao/prosodic_embeddings/bert_train/pretrained-bert/checkpoint-3000/model.safetensors\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask, title, description, url, image_url, text, domain, date. If special_tokens_mask, title, description, url, image_url, text, domain, date are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8221\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /shared/3/projects/bangzhao/prosodic_embeddings/bert_train/pretrained-bert/checkpoint-4000\n",
      "Configuration saved in /shared/3/projects/bangzhao/prosodic_embeddings/bert_train/pretrained-bert/checkpoint-4000/config.json\n",
      "Model weights saved in /shared/3/projects/bangzhao/prosodic_embeddings/bert_train/pretrained-bert/checkpoint-4000/model.safetensors\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask, title, description, url, image_url, text, domain, date. If special_tokens_mask, title, description, url, image_url, text, domain, date are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8221\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /shared/3/projects/bangzhao/prosodic_embeddings/bert_train/pretrained-bert/checkpoint-5000\n",
      "Configuration saved in /shared/3/projects/bangzhao/prosodic_embeddings/bert_train/pretrained-bert/checkpoint-5000/config.json\n",
      "Model weights saved in /shared/3/projects/bangzhao/prosodic_embeddings/bert_train/pretrained-bert/checkpoint-5000/model.safetensors\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask, title, description, url, image_url, text, domain, date. If special_tokens_mask, title, description, url, image_url, text, domain, date are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8221\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /shared/3/projects/bangzhao/prosodic_embeddings/bert_train/pretrained-bert/checkpoint-6000\n",
      "Configuration saved in /shared/3/projects/bangzhao/prosodic_embeddings/bert_train/pretrained-bert/checkpoint-6000/config.json\n",
      "Model weights saved in /shared/3/projects/bangzhao/prosodic_embeddings/bert_train/pretrained-bert/checkpoint-6000/model.safetensors\n",
      "Saving model checkpoint to /shared/3/projects/bangzhao/prosodic_embeddings/bert_train/pretrained-bert/checkpoint-6930\n",
      "Configuration saved in /shared/3/projects/bangzhao/prosodic_embeddings/bert_train/pretrained-bert/checkpoint-6930/config.json\n",
      "Model weights saved in /shared/3/projects/bangzhao/prosodic_embeddings/bert_train/pretrained-bert/checkpoint-6930/model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6930, training_loss=4.062674280472132, metrics={'train_runtime': 945.2284, 'train_samples_per_second': 234.585, 'train_steps_per_second': 7.332, 'total_flos': 302502114017280.0, 'train_loss': 4.062674280472132, 'epoch': 3.0})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3d1158d4-a3d5-44fe-8726-448b52830e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /shared/3/projects/bangzhao/prosodic_embeddings/bert_train/pretrained-bert/checkpoint-6930/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/shared/3/projects/bangzhao/prosodic_embeddings/bert_train/pretrained-bert/checkpoint-6930\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file /shared/3/projects/bangzhao/prosodic_embeddings/bert_train/pretrained-bert/checkpoint-6930/model.safetensors\n",
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at /shared/3/projects/bangzhao/prosodic_embeddings/bert_train/pretrained-bert/checkpoint-6930.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "loading configuration file /shared/3/projects/bangzhao/prosodic_embeddings/bert_train/pretrained-bert/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/shared/3/projects/bangzhao/prosodic_embeddings/bert_train/pretrained-bert/\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file /shared/3/projects/bangzhao/prosodic_embeddings/bert_train/pretrained-bert/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/shared/3/projects/bangzhao/prosodic_embeddings/bert_train/pretrained-bert/\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained(os.path.join(model_path, \"checkpoint-6930\"))\n",
    "# load the tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ce3a1d5a-db32-4af1-a63f-5ce31b0b28fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4b763b11-fdd7-4af8-829f-d045fe1efcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "today's most trending hashtags on twitter is donald trump, confidence: 0.2891486883163452\n",
      "today's most trending hashtags on facebook is donald trump, confidence: 0.08995313942432404\n",
      "today's most trending hashtags on trump is donald trump, confidence: 0.06772739440202713\n",
      "today's most trending hashtags on what is donald trump, confidence: 0.03804310783743858\n",
      "today's most trending hashtags on today is donald trump, confidence: 0.037062812596559525\n",
      "==================================================\n",
      "the evening was cloudy yesterday, but today it's rainy., confidence: 0.07923946529626846\n",
      "the morning was cloudy yesterday, but today it's rainy., confidence: 0.07475239038467407\n",
      "the weather was cloudy yesterday, but today it's rainy., confidence: 0.07051657140254974\n",
      "the storm was cloudy yesterday, but today it's rainy., confidence: 0.06587118655443192\n",
      "the afternoon was cloudy yesterday, but today it's rainy., confidence: 0.052087776362895966\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# perform predictions\n",
    "examples = [\n",
    "  \"Today's most trending hashtags on [MASK] is Donald Trump\",\n",
    "  \"The [MASK] was cloudy yesterday, but today it's rainy.\",\n",
    "]\n",
    "for example in examples:\n",
    "  for prediction in fill_mask(example):\n",
    "    print(f\"{prediction['sequence']}, confidence: {prediction['score']}\")\n",
    "  print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca76531-4caf-42f6-8f52-c81dd802aaa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
